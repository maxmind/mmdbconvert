# Querying Parquet Files

This guide explains how to efficiently query Parquet files generated by
`mmdbconvert` for IP address lookups.

## Why Structure Matters

The structure of your Parquet file dramatically affects query performance. With
proper configuration, IP lookups can be **10-100x faster** than naive
approaches.

**Key Insight:** Query engines use row group statistics to skip reading most of
the data file. Integer columns enable this optimization, while string-based
comparisons do not.

## Understanding Row Group Statistics

### What Are Row Groups?

Parquet files are divided into **row groups** — chunks of approximately 500,000
rows (configurable via `row_group_size` in your config).

Each row group stores **min/max statistics** for every column:

```
Row Group 1: start_int [0           ... 1,000,000,000]
Row Group 2: start_int [1,000,000,001 ... 2,000,000,000]
Row Group 3: start_int [2,000,000,001 ... 3,000,000,000]
Row Group 4: start_int [3,000,000,001 ... 4,294,967,295]
```

### Predicate Pushdown

When you query:

```sql
WHERE start_int <= 3405803876 AND end_int >= 3405803876
```

The query engine:

1. **Checks statistics** for each row group
2. **Skips row groups** that cannot possibly contain the IP
3. **Scans only** the relevant row group(s)

**Example:**

- Looking up IP `203.0.113.100` (integer: 3,405,803,876)
- Row Group 1: `start_int` max = 1,000,000,000 → **SKIP** (too low)
- Row Group 2: `start_int` max = 2,000,000,000 → **SKIP** (too low)
- Row Group 3: `start_int` min = 2,000,000,001, max = 3,000,000,000 → **SKIP**
  (too low)
- Row Group 4: `start_int` min = 3,000,000,001 → **SCAN** (might contain IP)

**Result:** Read only 1 out of 4 row groups (75% data skipping)!

### Why Integer Columns Are Critical

| Column Type | Predicate Pushdown | Performance        |
| ----------- | ------------------ | ------------------ |
| `start_int` | ✅ Yes             | **Fast** (10-100x) |
| `end_int`   | ✅ Yes             | **Fast** (10-100x) |
| `start_ip`  | ❌ No              | Slow (full scan)   |
| `end_ip`    | ❌ No              | Slow (full scan)   |
| `network`   | ❌ No              | Slow (full scan)   |

**Always use integer columns (`start_int`, `end_int`) in your WHERE clauses for
best performance.**

## Recommended Configuration

### Optimized for Queries

```toml
[output]
format = "parquet"
file = "geo.parquet"

[output.parquet]
compression = "snappy"
row_group_size = 500000

# INTEGER COLUMNS FIRST (critical for performance)
[[network.columns]]
name = "start_int"
type = "start_int"

[[network.columns]]
name = "end_int"
type = "end_int"

# Optional: human-readable CIDR (for reference)
[[network.columns]]
name = "network"
type = "cidr"
```

## Query Patterns by Engine

### DuckDB (Recommended)

DuckDB is the recommended query engine for Parquet files. It has excellent
predicate pushdown support and is easy to use.

#### Basic IP Lookup

```sql
-- Lookup single IP (203.0.113.100)
SELECT * FROM read_parquet('geo.parquet')
WHERE start_int <= 3405803876 AND end_int >= 3405803876;
```

#### Helper Function for IP Conversion

```sql
-- Create a function to convert IP string to integer
CREATE OR REPLACE FUNCTION ip_to_int(ip VARCHAR) AS
  CAST(split_part(ip, '.', 1) AS BIGINT) * 16777216 +
  CAST(split_part(ip, '.', 2) AS BIGINT) * 65536 +
  CAST(split_part(ip, '.', 3) AS BIGINT) * 256 +
  CAST(split_part(ip, '.', 4) AS BIGINT);

-- Use the function
SELECT * FROM read_parquet('geo.parquet')
WHERE start_int <= ip_to_int('203.0.113.100')
  AND end_int >= ip_to_int('203.0.113.100');
```

#### Batch Lookup with JOIN

```sql
-- Lookup multiple IPs from a table
CREATE TABLE access_logs (
  ip VARCHAR,
  timestamp TIMESTAMP,
  user_id INTEGER
);

-- Join with GeoIP data
SELECT
  a.ip,
  a.timestamp,
  a.user_id,
  g.country_code,
  g.city_name
FROM access_logs a
LEFT JOIN (
  SELECT * FROM read_parquet('geo.parquet')
) g
ON ip_to_int(a.ip) BETWEEN g.start_int AND g.end_int;
```

#### Multi-Column Filtering

```sql
-- Find all US IPs that are anonymous
SELECT
  network,
  city_name
FROM read_parquet('geo.parquet')
WHERE country_code = 'US'
  AND is_anonymous = true
ORDER BY start_int;
```

### Apache Spark

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .appName("GeoIP Lookup")
  .getOrCreate()

// Read Parquet file
val geoData = spark.read.parquet("geo.parquet")

// Create UDF for IP to integer conversion
val ipToInt = udf((ip: String) => {
  val parts = ip.split('.')
  parts(0).toLong * 16777216 +
  parts(1).toLong * 65536 +
  parts(2).toLong * 256 +
  parts(3).toLong
})

// Register as temp view
geoData.createOrReplaceTempView("geo")

// Query with predicate pushdown
val result = spark.sql("""
  SELECT * FROM geo
  WHERE start_int <= 3405803876 AND end_int >= 3405803876
""")

result.show()

// Batch lookup with DataFrame join
val accessLogs = spark.read.parquet("access_logs.parquet")
  .withColumn("ip_int", ipToInt(col("ip")))

val enriched = accessLogs.join(geoData,
  accessLogs("ip_int") >= geoData("start_int") &&
  accessLogs("ip_int") <= geoData("end_int"),
  "left"
)

enriched.write.parquet("enriched_logs.parquet")
```

**Performance Tip:** Enable predicate pushdown in Spark configuration:

```scala
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
spark.conf.set("spark.sql.parquet.enableVectorizedReader", "true")
```

### Trino/Presto

```sql
-- Single IP lookup
SELECT * FROM hive.default.geo_parquet
WHERE start_int <= 3405803876 AND end_int >= 3405803876;

-- Use CAST for IP conversion
CREATE FUNCTION ip_to_int(ip VARCHAR)
RETURNS BIGINT
RETURN
  CAST(split_part(ip, '.', 1) AS BIGINT) * 16777216 +
  CAST(split_part(ip, '.', 2) AS BIGINT) * 65536 +
  CAST(split_part(ip, '.', 3) AS BIGINT) * 256 +
  CAST(split_part(ip, '.', 4) AS BIGINT);

-- Batch lookup
SELECT
  a.ip,
  g.country_code,
  g.city_name
FROM access_logs a
LEFT JOIN geo_parquet g
  ON ip_to_int(a.ip) BETWEEN g.start_int AND g.end_int;
```

**Note:** Trino has native `IPADDRESS` type, but using integer comparison is
still faster for range queries.

## IP Address Conversion

### IPv4 String to Integer

The formula for converting an IPv4 address to an integer:

```
ip_int = octet1 * 16777216 + octet2 * 65536 + octet3 * 256 + octet4
```

**Example:** `203.0.113.100`

```
203 * 16777216 = 3,405,774,848
0   * 65536    =              0
113 * 256      =         28,928
100            =            100
                 ───────────────
                  3,405,803,876
```

### Implementation Examples

**Python:**

```python
def ip_to_int(ip: str) -> int:
    parts = [int(x) for x in ip.split('.')]
    return parts[0] * 16777216 + parts[1] * 65536 + parts[2] * 256 + parts[3]

# Or use standard library
import socket
import struct

def ip_to_int(ip: str) -> int:
    return struct.unpack("!I", socket.inet_aton(ip))[0]
```

**JavaScript/TypeScript:**

```javascript
function ipToInt(ip: string): number {
  const parts = ip.split('.').map(Number);
  return parts[0] * 16777216 + parts[1] * 65536 + parts[2] * 256 + parts[3];
}
```

**Go:**

```go
func ipToInt(ip string) (uint32, error) {
    addr, err := netip.ParseAddr(ip)
    if err != nil {
        return 0, err
    }
    if !addr.Is4() {
        return 0, fmt.Errorf("not an IPv4 address")
    }
    b := addr.As4()
    return uint32(b[0])<<24 | uint32(b[1])<<16 | uint32(b[2])<<8 | uint32(b[3]), nil
}
```

### IPv6 Handling

IPv6 addresses use 16-byte binary representation in Parquet (not human-readable
integers). For IPv6 queries, use `start_ip` and `end_ip` string columns instead
of integer columns:

```sql
-- IPv6 lookup (slower, but works)
SELECT * FROM read_parquet('geo.parquet')
WHERE start_ip <= '2001:db8::1' AND end_ip >= '2001:db8::1';
```

**Recommendation:** For better IPv6 performance, write IPv4 and IPv6 rows to
dedicated files:

```toml
[output]
format = "parquet"
ipv4_file = "geo_ipv4.parquet"
ipv6_file = "geo_ipv6.parquet"
```

## Common Query Patterns

### Single IP Lookup

```sql
-- Basic lookup
SELECT * FROM read_parquet('geo.parquet')
WHERE start_int <= 3405803876 AND end_int >= 3405803876;
```

### Batch IP Lookup

```sql
-- From CSV file
CREATE TABLE ips AS
  SELECT column0 AS ip
  FROM read_csv('ips.txt', header=false);

-- Enrich with GeoIP data
SELECT
  i.ip,
  g.*
FROM ips i
LEFT JOIN read_parquet('geo.parquet') g
  ON ip_to_int(i.ip) BETWEEN g.start_int AND g.end_int;
```

### Filter by Geographic Region

```sql
-- All networks in a specific country
SELECT
  network,
  city_name
FROM read_parquet('geo.parquet')
WHERE country_code = 'US'
ORDER BY start_int;

-- Networks in specific cities
SELECT
  network,
  country_code
FROM read_parquet('geo.parquet')
WHERE city_name IN ('London', 'Paris', 'Berlin');
```

### Anonymous IP Detection

```sql
-- Find anonymous IPs in access logs
SELECT
  a.timestamp,
  a.user_id,
  g.network,
  g.country_code
FROM access_logs a
JOIN read_parquet('geo.parquet') g
  ON ip_to_int(a.ip) BETWEEN g.start_int AND g.end_int
WHERE g.is_anonymous = true;
```

### Statistics and Aggregations

```sql
-- Count networks per country
SELECT
  country_code,
  COUNT(*) as network_count,
  SUM(end_int - start_int + 1) as total_ips
FROM read_parquet('geo.parquet')
GROUP BY country_code
ORDER BY total_ips DESC
LIMIT 10;

-- Find largest network blocks
SELECT
  network,
  country_code,
  (end_int - start_int + 1) as ip_count
FROM read_parquet('geo.parquet')
ORDER BY ip_count DESC
LIMIT 20;
```

### Time-Series Log Analysis

```sql
-- Analyze traffic by country over time
SELECT
  DATE_TRUNC('hour', a.timestamp) as hour,
  g.country_code,
  COUNT(*) as request_count,
  COUNT(DISTINCT a.user_id) as unique_users
FROM access_logs a
LEFT JOIN read_parquet('geo.parquet') g
  ON ip_to_int(a.ip) BETWEEN g.start_int AND g.end_int
GROUP BY 1, 2
ORDER BY 1, 3 DESC;
```

## Performance Characteristics

### Query Performance Benchmarks

Typical performance for a 5 million network database:

| Query Type                    | Integer Columns | String Columns | Improvement  |
| ----------------------------- | --------------- | -------------- | ------------ |
| Single IP lookup (cold)       | 50-100ms        | 2-5 seconds    | **40-50x**   |
| Single IP lookup (warm)       | 5-10ms          | 200-500ms      | **40-50x**   |
| Batch 1000 IPs                | 100-200ms       | 30-60 seconds  | **200-300x** |
| Full scan (filter by country) | 200-500ms       | 200-500ms      | Same         |

**Note:** "Cold" means first query (disk read). "Warm" means cached in memory.

### Row Group Statistics

For a 5 million network file with 500,000 rows per group:

- **Total row groups:** ~10
- **Row groups scanned per single IP lookup:** 1-2 (90% skipped)
- **Row groups scanned per batch lookup:** 1-3 (70% skipped)

### Memory Usage

Query engines typically load one row group at a time:

- **Row group size:** ~200-250 MB (uncompressed)
- **Memory per query:** 200-500 MB (depends on engine)
- **Concurrent queries:** Scales linearly with available memory

## Optimization Checklist

✅ **Use integer columns first in schema**

```toml
[[network.columns]]
name = "start_int"
type = "start_int"

[[network.columns]]
name = "end_int"
type = "end_int"
```

✅ **Always use WHERE on integer columns**

```sql
WHERE start_int <= {ip_int} AND end_int >= {ip_int}
```

✅ **Enable predicate pushdown (engine-specific)**

```scala
// Spark
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
```

✅ **Use appropriate compression**

- `snappy` — Best for query performance (default)
- `zstd` — Best compression ratio (slightly slower queries)
- `gzip` — Good balance (widely supported)
- `none` — Fastest queries, largest files

✅ **Consider separate IPv4/IPv6 files for large datasets**

```toml
[output]
format = "parquet"
ipv4_file = "geo_ipv4.parquet"
ipv6_file = "geo_ipv6.parquet"
```

✅ **Sorting metadata is automatic**

When you configure `start_int` columns, mmdbconvert automatically writes sorting
metadata to the Parquet file. This tells query engines that rows are sorted by
`start_int`, enabling optimizations like binary search. You can verify sorting
metadata is present:

```bash
# Using parquet-go (via Go code)
# Each row group will show: Path: [start_int], Descending: false

# Using DuckDB - check min/max statistics show non-overlapping ranges
duckdb -c "SELECT row_group_id, stats_min_value, stats_max_value
           FROM parquet_metadata('geo.parquet')
           WHERE path_in_schema = 'start_int'
           ORDER BY row_group_id;"
```

✅ **Tune row group size for your use case**

- Default: 500,000 rows (~250 MB)
- Larger datasets: Increase to 1,000,000
- Smaller datasets: Decrease to 100,000

## Troubleshooting

### Slow Queries

**Problem:** Queries take seconds instead of milliseconds

**Solutions:**

1. **Verify integer columns are being used:**

   ```sql
   -- Good (fast)
   WHERE start_int <= 3405803876 AND end_int >= 3405803876

   -- Bad (slow)
   WHERE start_ip <= '203.0.113.100' AND end_ip >= '203.0.113.100'
   ```

2. **Check if predicate pushdown is enabled**
3. **Examine query plan** (DuckDB: `EXPLAIN`, Spark: `.explain()`)
4. **Verify row group statistics** with parquet-tools:
   ```bash
   parquet-tools meta geo.parquet | grep -A 5 "row group"
   ```

### High Memory Usage

**Problem:** Queries consume excessive memory

**Solutions:**

1. **Reduce row group size:**

   ```toml
   [output.parquet]
   row_group_size = 250000  # Smaller groups
   ```

2. **Use streaming queries** (depends on engine)
3. **Limit concurrent queries**
4. **Increase available memory**

### Missing or Incorrect Data

**Problem:** Some IPs return no results or wrong data

**Solutions:**

1. **Verify IP is in the database:**

   ```sql
   SELECT MIN(start_int), MAX(end_int)
   FROM read_parquet('geo.parquet');
   ```

2. **Check for IPv4 vs IPv6 mismatch**
3. **Review column mapping** in your config file
4. **Verify MMDB source data** is correct

### Parquet File Corruption

**Problem:** Can't read Parquet file

**Solutions:**

1. **Validate file with parquet-tools:**

   ```bash
   parquet-tools schema geo.parquet
   parquet-tools cat --limit 10 geo.parquet
   ```

2. **Check file size** (empty or truncated?)
3. **Regenerate file** with `mmdbconvert`
4. **Verify disk space** during generation

## Additional Resources

- [Apache Parquet Documentation](https://parquet.apache.org/docs/)
- [DuckDB Parquet Guide](https://duckdb.org/docs/data/parquet)
- [Configuration Reference](config.md)
- [mmdbconvert GitHub Repository](https://github.com/maxmind/mmdbconvert)

## Summary

**Key Takeaways:**

1. **Always use integer columns** (`start_int`, `end_int`) for WHERE clauses
2. **Integer columns enable predicate pushdown** → 10-100x faster queries
3. **Row group statistics** allow query engines to skip most data
4. **Sorting metadata is automatic** when using `start_int` columns
5. **Compression matters:** Use `snappy` for query-optimized files

With proper configuration, you can achieve **millisecond query times** on
multi-million row GeoIP databases!
